{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>referenced_tweets.replied_to.id</th>\n",
       "      <th>referenced_tweets.retweeted.id</th>\n",
       "      <th>referenced_tweets.quoted.id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>retweeted_user_id</th>\n",
       "      <th>quoted_user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>...</th>\n",
       "      <th>geo.geo.bbox</th>\n",
       "      <th>geo.geo.type</th>\n",
       "      <th>geo.id</th>\n",
       "      <th>geo.name</th>\n",
       "      <th>geo.place_id</th>\n",
       "      <th>geo.place_type</th>\n",
       "      <th>__twarc.retrieved_at</th>\n",
       "      <th>__twarc.url</th>\n",
       "      <th>__twarc.version</th>\n",
       "      <th>Unnamed: 73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1410152713804029952</td>\n",
       "      <td>1410152713804029952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162218042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-30T08:26:20.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>[-106.645646, 25.837092, -93.508131, 36.500695]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>e0060cda70f5f341</td>\n",
       "      <td>Texas</td>\n",
       "      <td>e0060cda70f5f341</td>\n",
       "      <td>admin</td>\n",
       "      <td>2022-04-21T00:39:33+00:00</td>\n",
       "      <td>https://api.twitter.com/2/tweets/search/all?ex...</td>\n",
       "      <td>2.10.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1410152710461136897</td>\n",
       "      <td>1409965094780407810</td>\n",
       "      <td>1.409965e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>952415351106498560</td>\n",
       "      <td>4.546341e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-30T08:26:20.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>[-86.339696, 43.201538, -86.144095, 43.264395]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>012b416c2c776945</td>\n",
       "      <td>Muskegon</td>\n",
       "      <td>012b416c2c776945</td>\n",
       "      <td>city</td>\n",
       "      <td>2022-04-21T00:39:33+00:00</td>\n",
       "      <td>https://api.twitter.com/2/tweets/search/all?ex...</td>\n",
       "      <td>2.10.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1410152696750002179</td>\n",
       "      <td>1409603818585735169</td>\n",
       "      <td>1.409944e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>936561177819320320</td>\n",
       "      <td>1.475841e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-30T08:26:16.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>[-87.940033, 41.644102, -87.523993, 42.0230669]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>1d9a5370a355ab0c</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>1d9a5370a355ab0c</td>\n",
       "      <td>city</td>\n",
       "      <td>2022-04-21T00:39:33+00:00</td>\n",
       "      <td>https://api.twitter.com/2/tweets/search/all?ex...</td>\n",
       "      <td>2.10.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1410152691687432195</td>\n",
       "      <td>1410152691687432195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.410077e+18</td>\n",
       "      <td>496542587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.007085e+18</td>\n",
       "      <td>2021-06-30T08:26:15.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>[-86.505805, 35.751433, -86.313415, 35.943407]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>19e2bff2e89dc38e</td>\n",
       "      <td>Murfreesboro</td>\n",
       "      <td>19e2bff2e89dc38e</td>\n",
       "      <td>city</td>\n",
       "      <td>2022-04-21T00:39:33+00:00</td>\n",
       "      <td>https://api.twitter.com/2/tweets/search/all?ex...</td>\n",
       "      <td>2.10.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1410152678798237697</td>\n",
       "      <td>1410147768329551873</td>\n",
       "      <td>1.410148e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31366271</td>\n",
       "      <td>2.215141e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-30T08:26:12.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>[-88.097892, 37.771743, -84.78458, 41.761368]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>1010ecfa7d3a40f8</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>1010ecfa7d3a40f8</td>\n",
       "      <td>admin</td>\n",
       "      <td>2022-04-21T00:39:33+00:00</td>\n",
       "      <td>https://api.twitter.com/2/tweets/search/all?ex...</td>\n",
       "      <td>2.10.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1410152350308868101</td>\n",
       "      <td>1410152350308868101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2937943430</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-30T08:24:54.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>[-74.026675, 40.683935, -73.910408, 40.877483]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>01a9a39529b27f36</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>01a9a39529b27f36</td>\n",
       "      <td>city</td>\n",
       "      <td>2022-04-21T00:39:33+00:00</td>\n",
       "      <td>https://api.twitter.com/2/tweets/search/all?ex...</td>\n",
       "      <td>2.10.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1410152349688008705</td>\n",
       "      <td>1410152349688008705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1563271657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-30T08:24:54.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>[-83.5218655, 33.877554, -83.277693, 34.035213]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>01f9c9fd7bb1aa6b</td>\n",
       "      <td>Athens</td>\n",
       "      <td>01f9c9fd7bb1aa6b</td>\n",
       "      <td>city</td>\n",
       "      <td>2022-04-21T00:39:33+00:00</td>\n",
       "      <td>https://api.twitter.com/2/tweets/search/all?ex...</td>\n",
       "      <td>2.10.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1410152345988599811</td>\n",
       "      <td>1410150393263190018</td>\n",
       "      <td>1.410150e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1237113976238071808</td>\n",
       "      <td>1.291527e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-30T08:24:53.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>[-118.668404, 33.704538, -118.155409, 34.337041]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>3b77caf94bfc81fe</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>3b77caf94bfc81fe</td>\n",
       "      <td>city</td>\n",
       "      <td>2022-04-21T00:39:33+00:00</td>\n",
       "      <td>https://api.twitter.com/2/tweets/search/all?ex...</td>\n",
       "      <td>2.10.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1410152344512204801</td>\n",
       "      <td>1409741661895286790</td>\n",
       "      <td>1.409742e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26972692</td>\n",
       "      <td>7.592510e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-30T08:24:52.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>[-112.6265495, 33.355798, -112.461428, 33.5154...</td>\n",
       "      <td>Feature</td>\n",
       "      <td>0015cc0d71d49e19</td>\n",
       "      <td>Buckeye</td>\n",
       "      <td>0015cc0d71d49e19</td>\n",
       "      <td>city</td>\n",
       "      <td>2022-04-21T00:39:33+00:00</td>\n",
       "      <td>https://api.twitter.com/2/tweets/search/all?ex...</td>\n",
       "      <td>2.10.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1410152338908758017</td>\n",
       "      <td>1410105776035799042</td>\n",
       "      <td>1.410106e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5408562</td>\n",
       "      <td>8.026465e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-30T08:24:51.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>[-78.0280841, 39.426663, -77.94245, 39.494633]</td>\n",
       "      <td>Feature</td>\n",
       "      <td>d5039bee9eb93f2c</td>\n",
       "      <td>Martinsburg</td>\n",
       "      <td>d5039bee9eb93f2c</td>\n",
       "      <td>city</td>\n",
       "      <td>2022-04-21T00:39:33+00:00</td>\n",
       "      <td>https://api.twitter.com/2/tweets/search/all?ex...</td>\n",
       "      <td>2.10.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id      conversation_id  referenced_tweets.replied_to.id  \\\n",
       "0   1410152713804029952  1410152713804029952                              NaN   \n",
       "1   1410152710461136897  1409965094780407810                     1.409965e+18   \n",
       "2   1410152696750002179  1409603818585735169                     1.409944e+18   \n",
       "3   1410152691687432195  1410152691687432195                              NaN   \n",
       "4   1410152678798237697  1410147768329551873                     1.410148e+18   \n",
       "..                  ...                  ...                              ...   \n",
       "95  1410152350308868101  1410152350308868101                              NaN   \n",
       "96  1410152349688008705  1410152349688008705                              NaN   \n",
       "97  1410152345988599811  1410150393263190018                     1.410150e+18   \n",
       "98  1410152344512204801  1409741661895286790                     1.409742e+18   \n",
       "99  1410152338908758017  1410105776035799042                     1.410106e+18   \n",
       "\n",
       "    referenced_tweets.retweeted.id  referenced_tweets.quoted.id  \\\n",
       "0                              NaN                          NaN   \n",
       "1                              NaN                          NaN   \n",
       "2                              NaN                          NaN   \n",
       "3                              NaN                 1.410077e+18   \n",
       "4                              NaN                          NaN   \n",
       "..                             ...                          ...   \n",
       "95                             NaN                          NaN   \n",
       "96                             NaN                          NaN   \n",
       "97                             NaN                          NaN   \n",
       "98                             NaN                          NaN   \n",
       "99                             NaN                          NaN   \n",
       "\n",
       "              author_id  in_reply_to_user_id  retweeted_user_id  \\\n",
       "0             162218042                  NaN                NaN   \n",
       "1    952415351106498560         4.546341e+09                NaN   \n",
       "2    936561177819320320         1.475841e+09                NaN   \n",
       "3             496542587                  NaN                NaN   \n",
       "4              31366271         2.215141e+08                NaN   \n",
       "..                  ...                  ...                ...   \n",
       "95           2937943430                  NaN                NaN   \n",
       "96           1563271657                  NaN                NaN   \n",
       "97  1237113976238071808         1.291527e+18                NaN   \n",
       "98             26972692         7.592510e+05                NaN   \n",
       "99              5408562         8.026465e+17                NaN   \n",
       "\n",
       "    quoted_user_id                created_at  ...  \\\n",
       "0              NaN  2021-06-30T08:26:20.000Z  ...   \n",
       "1              NaN  2021-06-30T08:26:20.000Z  ...   \n",
       "2              NaN  2021-06-30T08:26:16.000Z  ...   \n",
       "3     1.007085e+18  2021-06-30T08:26:15.000Z  ...   \n",
       "4              NaN  2021-06-30T08:26:12.000Z  ...   \n",
       "..             ...                       ...  ...   \n",
       "95             NaN  2021-06-30T08:24:54.000Z  ...   \n",
       "96             NaN  2021-06-30T08:24:54.000Z  ...   \n",
       "97             NaN  2021-06-30T08:24:53.000Z  ...   \n",
       "98             NaN  2021-06-30T08:24:52.000Z  ...   \n",
       "99             NaN  2021-06-30T08:24:51.000Z  ...   \n",
       "\n",
       "                                         geo.geo.bbox geo.geo.type  \\\n",
       "0     [-106.645646, 25.837092, -93.508131, 36.500695]      Feature   \n",
       "1      [-86.339696, 43.201538, -86.144095, 43.264395]      Feature   \n",
       "2     [-87.940033, 41.644102, -87.523993, 42.0230669]      Feature   \n",
       "3      [-86.505805, 35.751433, -86.313415, 35.943407]      Feature   \n",
       "4       [-88.097892, 37.771743, -84.78458, 41.761368]      Feature   \n",
       "..                                                ...          ...   \n",
       "95     [-74.026675, 40.683935, -73.910408, 40.877483]      Feature   \n",
       "96    [-83.5218655, 33.877554, -83.277693, 34.035213]      Feature   \n",
       "97   [-118.668404, 33.704538, -118.155409, 34.337041]      Feature   \n",
       "98  [-112.6265495, 33.355798, -112.461428, 33.5154...      Feature   \n",
       "99     [-78.0280841, 39.426663, -77.94245, 39.494633]      Feature   \n",
       "\n",
       "              geo.id      geo.name      geo.place_id  geo.place_type  \\\n",
       "0   e0060cda70f5f341         Texas  e0060cda70f5f341           admin   \n",
       "1   012b416c2c776945      Muskegon  012b416c2c776945            city   \n",
       "2   1d9a5370a355ab0c       Chicago  1d9a5370a355ab0c            city   \n",
       "3   19e2bff2e89dc38e  Murfreesboro  19e2bff2e89dc38e            city   \n",
       "4   1010ecfa7d3a40f8       Indiana  1010ecfa7d3a40f8           admin   \n",
       "..               ...           ...               ...             ...   \n",
       "95  01a9a39529b27f36     Manhattan  01a9a39529b27f36            city   \n",
       "96  01f9c9fd7bb1aa6b        Athens  01f9c9fd7bb1aa6b            city   \n",
       "97  3b77caf94bfc81fe   Los Angeles  3b77caf94bfc81fe            city   \n",
       "98  0015cc0d71d49e19       Buckeye  0015cc0d71d49e19            city   \n",
       "99  d5039bee9eb93f2c   Martinsburg  d5039bee9eb93f2c            city   \n",
       "\n",
       "         __twarc.retrieved_at  \\\n",
       "0   2022-04-21T00:39:33+00:00   \n",
       "1   2022-04-21T00:39:33+00:00   \n",
       "2   2022-04-21T00:39:33+00:00   \n",
       "3   2022-04-21T00:39:33+00:00   \n",
       "4   2022-04-21T00:39:33+00:00   \n",
       "..                        ...   \n",
       "95  2022-04-21T00:39:33+00:00   \n",
       "96  2022-04-21T00:39:33+00:00   \n",
       "97  2022-04-21T00:39:33+00:00   \n",
       "98  2022-04-21T00:39:33+00:00   \n",
       "99  2022-04-21T00:39:33+00:00   \n",
       "\n",
       "                                          __twarc.url  __twarc.version  \\\n",
       "0   https://api.twitter.com/2/tweets/search/all?ex...           2.10.2   \n",
       "1   https://api.twitter.com/2/tweets/search/all?ex...           2.10.2   \n",
       "2   https://api.twitter.com/2/tweets/search/all?ex...           2.10.2   \n",
       "3   https://api.twitter.com/2/tweets/search/all?ex...           2.10.2   \n",
       "4   https://api.twitter.com/2/tweets/search/all?ex...           2.10.2   \n",
       "..                                                ...              ...   \n",
       "95  https://api.twitter.com/2/tweets/search/all?ex...           2.10.2   \n",
       "96  https://api.twitter.com/2/tweets/search/all?ex...           2.10.2   \n",
       "97  https://api.twitter.com/2/tweets/search/all?ex...           2.10.2   \n",
       "98  https://api.twitter.com/2/tweets/search/all?ex...           2.10.2   \n",
       "99  https://api.twitter.com/2/tweets/search/all?ex...           2.10.2   \n",
       "\n",
       "    Unnamed: 73  \n",
       "0           NaN  \n",
       "1           NaN  \n",
       "2           NaN  \n",
       "3           NaN  \n",
       "4           NaN  \n",
       "..          ...  \n",
       "95          NaN  \n",
       "96          NaN  \n",
       "97          NaN  \n",
       "98          NaN  \n",
       "99          NaN  \n",
       "\n",
       "[100 rows x 74 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "datapath = './collected/'\n",
    "usfiles = glob.glob(datapath + 'us*.csv')\n",
    "df = pd.read_csv(usfiles[0])\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# descriptive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpfiles = glob.glob(datapath + 'jp*.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8770"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(usfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./collected/us_014509_01Jan2021.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1471856"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the total number of tweets for files starting with us\n",
    "def count_tweets(files):\n",
    "    count = 0\n",
    "    for file in files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # delete duplicate id\n",
    "            df = df.drop_duplicates(subset=['id'])\n",
    "            count += len(df)\n",
    "        except:\n",
    "            print(file)\n",
    "    return count\n",
    "\n",
    "count_tweets(usfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300324"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tweets(jpfiles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate entropy by word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91964da8f04049a6be6ad359eeaea58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion loadings: [0.9440009593963623]\n",
      "Emotion entropy: 1.292481250360578\n",
      "Emotion entropy: -0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "def get_emotion_loadings(tweet_text):\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    emotion_classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "    result = emotion_classifier(tweet_text)\n",
    "\n",
    "    # Extract emotion probabilities\n",
    "    emotion_probabilities = [entry[\"score\"] for entry in result]\n",
    "    return emotion_probabilities\n",
    "\n",
    "# Example usage\n",
    "tweet_text = \"I love this new phone! It's amazing.\"\n",
    "emotion_loadings = get_emotion_loadings(tweet_text)\n",
    "print(\"Emotion loadings:\", emotion_loadings)\n",
    "\n",
    "def emotion_entropy(post_emotions, threshold=0.5):\n",
    "    binary_emotions = [1 if emotion >= threshold else 0 for emotion in post_emotions]\n",
    "    probabilities = [emotion_count / len(post_emotions) for emotion_count in binary_emotions]\n",
    "    \n",
    "    entropy = -np.sum([p * np.log2(p) if p > 0 else 0 for p in probabilities])\n",
    "    return entropy\n",
    "\n",
    "# Example usage\n",
    "post_emotions = [0.7, 0.2, 0.8, 0.4, 0.6, 0.1]\n",
    "entropy = emotion_entropy(post_emotions)\n",
    "print(\"Emotion entropy:\", entropy)\n",
    "\n",
    "tweet_text = \"I love this new phone! It's amazing.\"\n",
    "emotion_loadings = get_emotion_loadings(tweet_text)\n",
    "entropy = emotion_entropy(emotion_loadings)\n",
    "print(\"Emotion loadings:\", emotion_loadings)\n",
    "print(\"Emotion entropy:\", entropy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_en_nrc_emotion_lexicon(txt_path):\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    data = [line.strip().split('\\t') for line in lines]\n",
    "    df = pd.DataFrame(data, columns=[\"word\", \"emotion\", \"value\"])\n",
    "    df[\"value\"] = df[\"value\"].astype(int)\n",
    "    \n",
    "    df = df[df[\"value\"] == 1]\n",
    "    emotion_dict = {emotion: set(df[df[\"emotion\"] == emotion][\"word\"]) for emotion in df[\"emotion\"].unique()}\n",
    "    \n",
    "    return emotion_dict\n",
    "\n",
    "path_to_english_nrc = \"./preprocessing/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "english_nrc_emotion_dict = load_en_nrc_emotion_lexicon(path_to_english_nrc)\n",
    "\n",
    "\n",
    "def load_jp_nrc_emotion_lexicon(file_path):\n",
    "    emotion_dict = {\"anger\": set(), \"anticipation\": set(), \"disgust\": set(), \"fear\": set(), \"joy\": set(),\n",
    "                    \"negative\": set(), \"positive\": set(), \"sadness\": set(), \"surprise\": set(), \"trust\": set()}\n",
    "\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        next(f)  # Skip the header line\n",
    "        for line in f:\n",
    "            split_line = line.strip().split(\"\\t\")\n",
    "            japanese_word = split_line[-1]\n",
    "            emotions = [emotion for emotion, value in zip(emotion_dict.keys(), split_line[1:-1]) if int(value) == 1]\n",
    "\n",
    "            for emotion in emotions:\n",
    "                emotion_dict[emotion].add(japanese_word.lower())\n",
    "\n",
    "    return emotion_dict\n",
    "\n",
    "path_to_japanese_nrc = \"./preprocessing/Japanese-NRC-EmoLex.txt\"\n",
    "japanese_nrc_emotion_dict = load_jp_nrc_emotion_lexicon(path_to_japanese_nrc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_loadings_english(tweet_text, eed):\n",
    "    tokenized_tweet = word_tokenize(tweet_text)\n",
    "\n",
    "    # Count the occurrences of words related to each emotion category\n",
    "    basic_emotions = ['fear', 'anger', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "    emotion_counts = {emotion: 0 for emotion in eed if emotion in basic_emotions}\n",
    "    for token in tokenized_tweet:\n",
    "        for emotion, words in eed.items():\n",
    "            if emotion in basic_emotions and token in words:\n",
    "                emotion_counts[emotion] += 1\n",
    "\n",
    "    # Calculate emotion loadings (normalize counts by the total number of emotion words)\n",
    "    total_emotion_words = sum(emotion_counts.values())\n",
    "    emotion_loadings = [count / total_emotion_words if total_emotion_words > 0 else 0 for count in emotion_counts.values()]\n",
    "\n",
    "    return emotion_loadings\n",
    "\n",
    "def get_emotion_loadings_japanese(tweet_text, jed):\n",
    "    tokenized_tweet = list(jieba.cut(tweet_text))\n",
    "\n",
    "    # Count the occurrences of words related to each emotion category\n",
    "    basic_emotions = ['fear', 'anger', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "    emotion_counts = {emotion: 0 for emotion in jed if emotion in basic_emotions}\n",
    "    for token in tokenized_tweet:\n",
    "        for emotion, words in jed.items():\n",
    "            if emotion in basic_emotions and token in words:\n",
    "                emotion_counts[emotion] += 1\n",
    "\n",
    "    # Calculate emotion loadings (normalize counts by the total number of emotion words)\n",
    "    total_emotion_words = sum(emotion_counts.values())\n",
    "    emotion_loadings = [count / total_emotion_words if total_emotion_words > 0 else 0 for count in emotion_counts.values()]\n",
    "\n",
    "    return emotion_loadings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English tweet emotion loadings: [0.0, 0.0, 0.0, 0.5, 0.0, 0.5]\n",
      "Japanese tweet emotion loadings: [0.0, 0.5, 0.5, 0.0, 0.0, 0.0]\n",
      "English tweet emotion entropy: 0.861654166907052\n",
      "Japanese tweet emotion entropy: 0.861654166907052\n"
     ]
    }
   ],
   "source": [
    "english_tweet_text = \"This new phone is amazing! It's simply magical.\"  # Replace with your English tweet text\n",
    "japanese_tweet_text = \"この新しい携帯電話は素晴らしいです！ただただ魔法のようです。\"  # Replace with your Japanese tweet text\n",
    "\n",
    "english_emotion_loadings = get_emotion_loadings_english(english_tweet_text, english_nrc_emotion_dict)\n",
    "japanese_emotion_loadings = get_emotion_loadings_japanese(japanese_tweet_text, japanese_nrc_emotion_dict)\n",
    "\n",
    "english_entropy = emotion_entropy(english_emotion_loadings)\n",
    "japanese_entropy = emotion_entropy(japanese_emotion_loadings)\n",
    "print(\"English tweet emotion loadings:\", english_emotion_loadings)\n",
    "print(\"Japanese tweet emotion loadings:\", japanese_emotion_loadings)\n",
    "print(\"English tweet emotion entropy:\", english_entropy)\n",
    "print(\"Japanese tweet emotion entropy:\", japanese_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I'm so happy to see you, but I'm sad that you have to leave soon.\n",
      "Emotion Loadings: [0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333]\n",
      "Entropy: 0.00\n",
      "\n",
      "Sentence: 彼の成功は素晴らしいけれども、同時に少し嫉妬心がある\n",
      "Emotion Loadings: [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "Entropy: 0.43\n",
      "\n",
      "Sentence: I'm anxious about the exam, but I'm also excited about the challenge.\n",
      "Emotion Loadings: [0.4, 0.0, 0.2, 0.2, 0.0, 0.2]\n",
      "Entropy: 0.00\n",
      "\n",
      "Sentence: 仕事が終わったので安心だが、週末のパーティには緊張している\n",
      "Emotion Loadings: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "Entropy: 0.43\n",
      "\n",
      "Sentence: I'm surprised and disappointed by the news.\n",
      "Emotion Loadings: [0.0, 0.25, 0.25, 0.25, 0.25, 0.0]\n",
      "Entropy: 0.00\n",
      "\n",
      "Sentence: 彼女の発表には驚いたが、ちょっと怒りも感じた\n",
      "Emotion Loadings: [0, 0, 0, 0, 0, 0]\n",
      "Entropy: 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mixed_emotion_sentences = [\n",
    "    \"I'm so happy to see you, but I'm sad that you have to leave soon.\",\n",
    "    \"彼の成功は素晴らしいけれども、同時に少し嫉妬心がある\",\n",
    "    \"I'm anxious about the exam, but I'm also excited about the challenge.\",\n",
    "    \"仕事が終わったので安心だが、週末のパーティには緊張している\",\n",
    "    \"I'm surprised and disappointed by the news.\",\n",
    "    \"彼女の発表には驚いたが、ちょっと怒りも感じた\"\n",
    "]\n",
    "english_emotion_loadings = [get_emotion_loadings_english(sentence, english_nrc_emotion_dict) for sentence in mixed_emotion_sentences[::2]]\n",
    "japanese_emotion_loadings = [get_emotion_loadings_japanese(sentence, japanese_nrc_emotion_dict) for sentence in mixed_emotion_sentences[1::2]]\n",
    "english_entropy = [emotion_entropy(loadings) for loadings in english_emotion_loadings]\n",
    "japanese_entropy = [emotion_entropy(loadings) for loadings in japanese_emotion_loadings]\n",
    "for i, sentence in enumerate(mixed_emotion_sentences):\n",
    "    if i % 2 == 0:\n",
    "        loadings = english_emotion_loadings[i // 2]\n",
    "        entropy = english_entropy[i // 2]\n",
    "    else:\n",
    "        loadings = japanese_emotion_loadings[i // 2]\n",
    "        entropy = japanese_entropy[i // 2]\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Emotion Loadings: {loadings}\")\n",
    "    print(f\"Entropy: {entropy:.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entropy by DDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_emotions = ['anger', 'fear', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "\n",
    "japanese_basic_emotion_concepts = {\n",
    "    emotion: list(words) for emotion, words in japanese_nrc_emotion_dict.items() if emotion in basic_emotions\n",
    "}\n",
    "english_basic_emotion_concepts = {\n",
    "    emotion: list(words) for emotion, words in english_nrc_emotion_dict.items() if emotion in basic_emotions\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "def load_glove_embeddings(embeddings_file):\n",
    "    embeddings = {}\n",
    "    with open(embeddings_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Vectorize the dictionary of moral and emotional concepts\n",
    "def vectorize_concepts(concepts, embeddings):\n",
    "    concept_vectors = {}\n",
    "    for concept, words in concepts.items():\n",
    "        vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "        concept_vectors[concept] = np.mean(vectors, axis=0)\n",
    "    return concept_vectors\n",
    "\n",
    "# Vectorize the input text\n",
    "def vectorize_text(text, embeddings):\n",
    "    tokens = word_tokenize(text)\n",
    "    vectors = [embeddings[token] for token in tokens if token in embeddings]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Calculate cosine similarity between input text and moral and emotional concepts\n",
    "def calculate_similarities(text_vector, concept_vectors):\n",
    "    similarities = {}\n",
    "    for concept, vector in concept_vectors.items():\n",
    "        similarities[concept] = cosine_similarity(text_vector.reshape(1, -1), vector.reshape(1, -1))[0][0]\n",
    "    return similarities\n",
    "\n",
    "# Example usage:\n",
    "glove_embeddings_file = 'glove.6B.300d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_embeddings_file)\n",
    "\n",
    "# Define your moral and emotional concepts dictionary here\n",
    "concepts = english_basic_emotion_concepts\n",
    "\n",
    "concept_vectors = vectorize_concepts(concepts, glove_embeddings)\n",
    "\n",
    "text = \"This is an example text to analyze moral and emotional content.\"\n",
    "text_vector = vectorize_text(text, glove_embeddings)\n",
    "\n",
    "similarities = calculate_similarities(text_vector, concept_vectors)\n",
    "print(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'janome'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/emotional_entropy.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Byeslab1.psych.ucsb.edu/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/emotional_entropy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Byeslab1.psych.ucsb.edu/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/emotional_entropy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Byeslab1.psych.ucsb.edu/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/emotional_entropy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjanome\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenizer\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Byeslab1.psych.ucsb.edu/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/emotional_entropy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Load pre-trained FastText embeddings\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Byeslab1.psych.ucsb.edu/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/emotional_entropy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_fasttext_embeddings\u001b[39m(embeddings_file):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'janome'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# Load pre-trained FastText embeddings\n",
    "def load_fasttext_embeddings(embeddings_file):\n",
    "    embeddings = {}\n",
    "    with open(embeddings_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Vectorize the dictionary of moral and emotional concepts\n",
    "def vectorize_concepts(concepts, embeddings):\n",
    "    concept_vectors = {}\n",
    "    for concept, words in concepts.items():\n",
    "        vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "        concept_vectors[concept] = np.mean(vectors, axis=0)\n",
    "    return concept_vectors\n",
    "\n",
    "# Tokenize the input text using Janome\n",
    "def tokenize_japanese_text(text):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokens = tokenizer.tokenize(text, wakati=True)\n",
    "    return tokens\n",
    "\n",
    "# Vectorize the input text\n",
    "def vectorize_text(tokens, embeddings):\n",
    "    vectors = [embeddings[token] for token in tokens if token in embeddings]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Calculate cosine similarity between input text and moral and emotional concepts\n",
    "def calculate_similarities(text_vector, concept_vectors):\n",
    "    similarities = {}\n",
    "    for concept, vector in concept_vectors.items():\n",
    "        similarities[concept] = cosine_similarity(text_vector.reshape(1, -1), vector.reshape(1, -1))[0][0]\n",
    "    return similarities\n",
    "\n",
    "# Example usage:\n",
    "fasttext_embeddings_file = 'cc.ja.300.vec'\n",
    "fasttext_embeddings = load_fasttext_embeddings(fasttext_embeddings_file)\n",
    "\n",
    "# Define your moral and emotional concepts dictionary here\n",
    "concepts = japanese_basic_emotion_concepts\n",
    "\n",
    "concept_vectors = vectorize_concepts(concepts, fasttext_embeddings)\n",
    "\n",
    "text = \"これは、道徳的および感情的なコンテンツを分析するための例文です。\"\n",
    "tokens = tokenize_japanese_text(text)\n",
    "text_vector = vectorize_text(tokens, fasttext_embeddings)\n",
    "\n",
    "similarities = calculate_similarities(text_vector, concept_vectors)\n",
    "print(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/twitter\n",
      "\n",
      "  added / updated specs:\n",
      "    - scikit-learn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    joblib-1.2.0               |     pyhd8ed1ab_0         205 KB  conda-forge\n",
      "    libblas-3.9.0              |   12_linux64_mkl          12 KB  conda-forge\n",
      "    libcblas-3.9.0             |   12_linux64_mkl          12 KB  conda-forge\n",
      "    libgfortran-ng-12.2.0      |      h69a702a_19          22 KB  conda-forge\n",
      "    libgfortran5-12.2.0        |      h337968e_19         1.8 MB  conda-forge\n",
      "    liblapack-3.9.0            |   12_linux64_mkl          12 KB  conda-forge\n",
      "    scikit-learn-0.24.2        |   py39h4dfa638_0         7.6 MB  conda-forge\n",
      "    scipy-1.5.3                |   py39hee8e79c_0        19.3 MB  conda-forge\n",
      "    threadpoolctl-3.1.0        |     pyh8a188c0_0          18 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        28.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  joblib             conda-forge/noarch::joblib-1.2.0-pyhd8ed1ab_0\n",
      "  libblas            conda-forge/linux-64::libblas-3.9.0-12_linux64_mkl\n",
      "  libcblas           conda-forge/linux-64::libcblas-3.9.0-12_linux64_mkl\n",
      "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-12.2.0-h69a702a_19\n",
      "  libgfortran5       conda-forge/linux-64::libgfortran5-12.2.0-h337968e_19\n",
      "  liblapack          conda-forge/linux-64::liblapack-3.9.0-12_linux64_mkl\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.9-2_cp39\n",
      "  scikit-learn       conda-forge/linux-64::scikit-learn-0.24.2-py39h4dfa638_0\n",
      "  scipy              conda-forge/linux-64::scipy-1.5.3-py39hee8e79c_0\n",
      "  threadpoolctl      conda-forge/noarch::threadpoolctl-3.1.0-pyh8a188c0_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2022.3.29-~ --> conda-forge::ca-certificates-2022.12.7-ha878542_0\n",
      "  certifi            pkgs/main/linux-64::certifi-2021.10.8~ --> conda-forge/noarch::certifi-2022.12.7-pyhd8ed1ab_0\n",
      "  openssl                                 1.1.1n-h7f8727e_0 --> 1.1.1t-h7f8727e_0\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? "
     ]
    }
   ],
   "source": [
    "!conda install scikit-learn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
