{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/collected/us_014509_01Jan2021.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# datapath = './collected/'\n",
    "datapath = '/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/collected/'\n",
    "preprocessedpath = '/home/local/PSYCH-ADS/xuqian_chen/Github/twitter_entropy/data/preprocessed/'\n",
    "# mkdir if not exist\n",
    "import os\n",
    "if not os.path.exists(preprocessedpath):\n",
    "    os.makedirs(preprocessedpath)\n",
    "\n",
    "usfiles = glob.glob(datapath + 'us*.csv')\n",
    "jpfiles = glob.glob(datapath + 'jp*.csv')\n",
    "\n",
    "# put us files together\n",
    "usdf = pd.DataFrame()\n",
    "for file in usfiles:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        usdf = pd.concat([usdf, df])\n",
    "    except:\n",
    "        print(file)\n",
    "usdf.to_csv(preprocessedpath + 'us.csv',index=False)\n",
    "\n",
    "# put jp files together\n",
    "jpdf = pd.DataFrame()\n",
    "for file in jpfiles:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        jpdf = pd.concat([jpdf, df])\n",
    "    except:\n",
    "        print(file)\n",
    "jpdf.to_csv(preprocessedpath + 'jp.csv',index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# descriptive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'usdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m usdf\n",
      "\u001b[0;31mNameError\u001b[0m: name 'usdf' is not defined"
     ]
    }
   ],
   "source": [
    "usdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8770"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(usfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./collected/us_014509_01Jan2021.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1471856"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the total number of tweets for files starting with us\n",
    "def count_tweets(files):\n",
    "    count = 0\n",
    "    for file in files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # delete duplicate id\n",
    "            df = df.drop_duplicates(subset=['id'])\n",
    "            count += len(df)\n",
    "        except:\n",
    "            print(file)\n",
    "    return count\n",
    "\n",
    "count_tweets(usfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300324"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tweets(jpfiles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate entropy by word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91964da8f04049a6be6ad359eeaea58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion loadings: [0.9440009593963623]\n",
      "Emotion entropy: 1.292481250360578\n",
      "Emotion entropy: -0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "def get_emotion_loadings(tweet_text):\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    emotion_classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "    result = emotion_classifier(tweet_text)\n",
    "\n",
    "    # Extract emotion probabilities\n",
    "    emotion_probabilities = [entry[\"score\"] for entry in result]\n",
    "    return emotion_probabilities\n",
    "\n",
    "# Example usage\n",
    "tweet_text = \"I love this new phone! It's amazing.\"\n",
    "emotion_loadings = get_emotion_loadings(tweet_text)\n",
    "print(\"Emotion loadings:\", emotion_loadings)\n",
    "\n",
    "def emotion_entropy(post_emotions, threshold=0.5):\n",
    "    binary_emotions = [1 if emotion >= threshold else 0 for emotion in post_emotions]\n",
    "    probabilities = [emotion_count / len(post_emotions) for emotion_count in binary_emotions]\n",
    "    \n",
    "    entropy = -np.sum([p * np.log2(p) if p > 0 else 0 for p in probabilities])\n",
    "    return entropy\n",
    "\n",
    "# Example usage\n",
    "post_emotions = [0.7, 0.2, 0.8, 0.4, 0.6, 0.1]\n",
    "entropy = emotion_entropy(post_emotions)\n",
    "print(\"Emotion entropy:\", entropy)\n",
    "\n",
    "tweet_text = \"I love this new phone! It's amazing.\"\n",
    "emotion_loadings = get_emotion_loadings(tweet_text)\n",
    "entropy = emotion_entropy(emotion_loadings)\n",
    "print(\"Emotion loadings:\", emotion_loadings)\n",
    "print(\"Emotion entropy:\", entropy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_en_nrc_emotion_lexicon(txt_path):\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    data = [line.strip().split('\\t') for line in lines]\n",
    "    df = pd.DataFrame(data, columns=[\"word\", \"emotion\", \"value\"])\n",
    "    df[\"value\"] = df[\"value\"].astype(int)\n",
    "    \n",
    "    df = df[df[\"value\"] == 1]\n",
    "    emotion_dict = {emotion: set(df[df[\"emotion\"] == emotion][\"word\"]) for emotion in df[\"emotion\"].unique()}\n",
    "    \n",
    "    return emotion_dict\n",
    "\n",
    "path_to_english_nrc = \"./preprocessing/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "english_nrc_emotion_dict = load_en_nrc_emotion_lexicon(path_to_english_nrc)\n",
    "\n",
    "\n",
    "def load_jp_nrc_emotion_lexicon(file_path):\n",
    "    emotion_dict = {\"anger\": set(), \"anticipation\": set(), \"disgust\": set(), \"fear\": set(), \"joy\": set(),\n",
    "                    \"negative\": set(), \"positive\": set(), \"sadness\": set(), \"surprise\": set(), \"trust\": set()}\n",
    "\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        next(f)  # Skip the header line\n",
    "        for line in f:\n",
    "            split_line = line.strip().split(\"\\t\")\n",
    "            japanese_word = split_line[-1]\n",
    "            emotions = [emotion for emotion, value in zip(emotion_dict.keys(), split_line[1:-1]) if int(value) == 1]\n",
    "\n",
    "            for emotion in emotions:\n",
    "                emotion_dict[emotion].add(japanese_word.lower())\n",
    "\n",
    "    return emotion_dict\n",
    "\n",
    "path_to_japanese_nrc = \"./preprocessing/Japanese-NRC-EmoLex.txt\"\n",
    "japanese_nrc_emotion_dict = load_jp_nrc_emotion_lexicon(path_to_japanese_nrc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_loadings_english(tweet_text, eed):\n",
    "    tokenized_tweet = word_tokenize(tweet_text)\n",
    "\n",
    "    # Count the occurrences of words related to each emotion category\n",
    "    basic_emotions = ['fear', 'anger', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "    emotion_counts = {emotion: 0 for emotion in eed if emotion in basic_emotions}\n",
    "    for token in tokenized_tweet:\n",
    "        for emotion, words in eed.items():\n",
    "            if emotion in basic_emotions and token in words:\n",
    "                emotion_counts[emotion] += 1\n",
    "\n",
    "    # Calculate emotion loadings (normalize counts by the total number of emotion words)\n",
    "    total_emotion_words = sum(emotion_counts.values())\n",
    "    emotion_loadings = [count / total_emotion_words if total_emotion_words > 0 else 0 for count in emotion_counts.values()]\n",
    "\n",
    "    return emotion_loadings\n",
    "\n",
    "def get_emotion_loadings_japanese(tweet_text, jed):\n",
    "    tokenized_tweet = list(jieba.cut(tweet_text))\n",
    "\n",
    "    # Count the occurrences of words related to each emotion category\n",
    "    basic_emotions = ['fear', 'anger', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "    emotion_counts = {emotion: 0 for emotion in jed if emotion in basic_emotions}\n",
    "    for token in tokenized_tweet:\n",
    "        for emotion, words in jed.items():\n",
    "            if emotion in basic_emotions and token in words:\n",
    "                emotion_counts[emotion] += 1\n",
    "\n",
    "    # Calculate emotion loadings (normalize counts by the total number of emotion words)\n",
    "    total_emotion_words = sum(emotion_counts.values())\n",
    "    emotion_loadings = [count / total_emotion_words if total_emotion_words > 0 else 0 for count in emotion_counts.values()]\n",
    "\n",
    "    return emotion_loadings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English tweet emotion loadings: [0.0, 0.0, 0.0, 0.5, 0.0, 0.5]\n",
      "Japanese tweet emotion loadings: [0.0, 0.5, 0.5, 0.0, 0.0, 0.0]\n",
      "English tweet emotion entropy: 0.861654166907052\n",
      "Japanese tweet emotion entropy: 0.861654166907052\n"
     ]
    }
   ],
   "source": [
    "english_tweet_text = \"This new phone is amazing! It's simply magical.\"  # Replace with your English tweet text\n",
    "japanese_tweet_text = \"この新しい携帯電話は素晴らしいです！ただただ魔法のようです。\"  # Replace with your Japanese tweet text\n",
    "\n",
    "english_emotion_loadings = get_emotion_loadings_english(english_tweet_text, english_nrc_emotion_dict)\n",
    "japanese_emotion_loadings = get_emotion_loadings_japanese(japanese_tweet_text, japanese_nrc_emotion_dict)\n",
    "\n",
    "english_entropy = emotion_entropy(english_emotion_loadings)\n",
    "japanese_entropy = emotion_entropy(japanese_emotion_loadings)\n",
    "print(\"English tweet emotion loadings:\", english_emotion_loadings)\n",
    "print(\"Japanese tweet emotion loadings:\", japanese_emotion_loadings)\n",
    "print(\"English tweet emotion entropy:\", english_entropy)\n",
    "print(\"Japanese tweet emotion entropy:\", japanese_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I'm so happy to see you, but I'm sad that you have to leave soon.\n",
      "Emotion Loadings: [0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333]\n",
      "Entropy: 0.00\n",
      "\n",
      "Sentence: 彼の成功は素晴らしいけれども、同時に少し嫉妬心がある\n",
      "Emotion Loadings: [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "Entropy: 0.43\n",
      "\n",
      "Sentence: I'm anxious about the exam, but I'm also excited about the challenge.\n",
      "Emotion Loadings: [0.4, 0.0, 0.2, 0.2, 0.0, 0.2]\n",
      "Entropy: 0.00\n",
      "\n",
      "Sentence: 仕事が終わったので安心だが、週末のパーティには緊張している\n",
      "Emotion Loadings: [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "Entropy: 0.43\n",
      "\n",
      "Sentence: I'm surprised and disappointed by the news.\n",
      "Emotion Loadings: [0.0, 0.25, 0.25, 0.25, 0.25, 0.0]\n",
      "Entropy: 0.00\n",
      "\n",
      "Sentence: 彼女の発表には驚いたが、ちょっと怒りも感じた\n",
      "Emotion Loadings: [0, 0, 0, 0, 0, 0]\n",
      "Entropy: 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mixed_emotion_sentences = [\n",
    "    \"I'm so happy to see you, but I'm sad that you have to leave soon.\",\n",
    "    \"彼の成功は素晴らしいけれども、同時に少し嫉妬心がある\",\n",
    "    \"I'm anxious about the exam, but I'm also excited about the challenge.\",\n",
    "    \"仕事が終わったので安心だが、週末のパーティには緊張している\",\n",
    "    \"I'm surprised and disappointed by the news.\",\n",
    "    \"彼女の発表には驚いたが、ちょっと怒りも感じた\"\n",
    "]\n",
    "english_emotion_loadings = [get_emotion_loadings_english(sentence, english_nrc_emotion_dict) for sentence in mixed_emotion_sentences[::2]]\n",
    "japanese_emotion_loadings = [get_emotion_loadings_japanese(sentence, japanese_nrc_emotion_dict) for sentence in mixed_emotion_sentences[1::2]]\n",
    "english_entropy = [emotion_entropy(loadings) for loadings in english_emotion_loadings]\n",
    "japanese_entropy = [emotion_entropy(loadings) for loadings in japanese_emotion_loadings]\n",
    "for i, sentence in enumerate(mixed_emotion_sentences):\n",
    "    if i % 2 == 0:\n",
    "        loadings = english_emotion_loadings[i // 2]\n",
    "        entropy = english_entropy[i // 2]\n",
    "    else:\n",
    "        loadings = japanese_emotion_loadings[i // 2]\n",
    "        entropy = japanese_entropy[i // 2]\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Emotion Loadings: {loadings}\")\n",
    "    print(f\"Entropy: {entropy:.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entropy by DDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_emotions = ['anger', 'fear', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "\n",
    "japanese_basic_emotion_concepts = {\n",
    "    emotion: list(words) for emotion, words in japanese_nrc_emotion_dict.items() if emotion in basic_emotions\n",
    "}\n",
    "english_basic_emotion_concepts = {\n",
    "    emotion: list(words) for emotion, words in english_nrc_emotion_dict.items() if emotion in basic_emotions\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "def load_glove_embeddings(embeddings_file):\n",
    "    embeddings = {}\n",
    "    with open(embeddings_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Vectorize the dictionary of moral and emotional concepts\n",
    "def vectorize_concepts(concepts, embeddings):\n",
    "    concept_vectors = {}\n",
    "    for concept, words in concepts.items():\n",
    "        vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "        concept_vectors[concept] = np.mean(vectors, axis=0)\n",
    "    return concept_vectors\n",
    "\n",
    "# Vectorize the input text\n",
    "def vectorize_text(text, embeddings):\n",
    "    tokens = word_tokenize(text)\n",
    "    vectors = [embeddings[token] for token in tokens if token in embeddings]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Calculate cosine similarity between input text and moral and emotional concepts\n",
    "def calculate_similarities(text_vector, concept_vectors):\n",
    "    similarities = {}\n",
    "    for concept, vector in concept_vectors.items():\n",
    "        similarities[concept] = cosine_similarity(text_vector.reshape(1, -1), vector.reshape(1, -1))[0][0]\n",
    "    return similarities\n",
    "\n",
    "# Example usage:\n",
    "glove_embeddings_file = 'glove.6B.300d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_embeddings_file)\n",
    "\n",
    "# Define your moral and emotional concepts dictionary here\n",
    "concepts = english_basic_emotion_concepts\n",
    "\n",
    "concept_vectors = vectorize_concepts(concepts, glove_embeddings)\n",
    "\n",
    "text = \"This is an example text to analyze moral and emotional content.\"\n",
    "text_vector = vectorize_text(text, glove_embeddings)\n",
    "\n",
    "similarities = calculate_similarities(text_vector, concept_vectors)\n",
    "print(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'janome'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/emotional_entropy.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Byeslab1.psych.ucsb.edu/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/emotional_entropy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Byeslab1.psych.ucsb.edu/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/emotional_entropy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpairwise\u001b[39;00m \u001b[39mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Byeslab1.psych.ucsb.edu/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/emotional_entropy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjanome\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenizer\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Byeslab1.psych.ucsb.edu/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/emotional_entropy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Load pre-trained FastText embeddings\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Byeslab1.psych.ucsb.edu/home/local/PSYCH-ADS/xuqian_chen/Projects/Twitter/emotional_entropy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_fasttext_embeddings\u001b[39m(embeddings_file):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'janome'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# Load pre-trained FastText embeddings\n",
    "def load_fasttext_embeddings(embeddings_file):\n",
    "    embeddings = {}\n",
    "    with open(embeddings_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Vectorize the dictionary of moral and emotional concepts\n",
    "def vectorize_concepts(concepts, embeddings):\n",
    "    concept_vectors = {}\n",
    "    for concept, words in concepts.items():\n",
    "        vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "        concept_vectors[concept] = np.mean(vectors, axis=0)\n",
    "    return concept_vectors\n",
    "\n",
    "# Tokenize the input text using Janome\n",
    "def tokenize_japanese_text(text):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokens = tokenizer.tokenize(text, wakati=True)\n",
    "    return tokens\n",
    "\n",
    "# Vectorize the input text\n",
    "def vectorize_text(tokens, embeddings):\n",
    "    vectors = [embeddings[token] for token in tokens if token in embeddings]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Calculate cosine similarity between input text and moral and emotional concepts\n",
    "def calculate_similarities(text_vector, concept_vectors):\n",
    "    similarities = {}\n",
    "    for concept, vector in concept_vectors.items():\n",
    "        similarities[concept] = cosine_similarity(text_vector.reshape(1, -1), vector.reshape(1, -1))[0][0]\n",
    "    return similarities\n",
    "\n",
    "# Example usage:\n",
    "fasttext_embeddings_file = 'cc.ja.300.vec'\n",
    "fasttext_embeddings = load_fasttext_embeddings(fasttext_embeddings_file)\n",
    "\n",
    "# Define your moral and emotional concepts dictionary here\n",
    "concepts = japanese_basic_emotion_concepts\n",
    "\n",
    "concept_vectors = vectorize_concepts(concepts, fasttext_embeddings)\n",
    "\n",
    "text = \"これは、道徳的および感情的なコンテンツを分析するための例文です。\"\n",
    "tokens = tokenize_japanese_text(text)\n",
    "text_vector = vectorize_text(tokens, fasttext_embeddings)\n",
    "\n",
    "similarities = calculate_similarities(text_vector, concept_vectors)\n",
    "print(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/twitter\n",
      "\n",
      "  added / updated specs:\n",
      "    - scikit-learn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    joblib-1.2.0               |     pyhd8ed1ab_0         205 KB  conda-forge\n",
      "    libblas-3.9.0              |   12_linux64_mkl          12 KB  conda-forge\n",
      "    libcblas-3.9.0             |   12_linux64_mkl          12 KB  conda-forge\n",
      "    libgfortran-ng-12.2.0      |      h69a702a_19          22 KB  conda-forge\n",
      "    libgfortran5-12.2.0        |      h337968e_19         1.8 MB  conda-forge\n",
      "    liblapack-3.9.0            |   12_linux64_mkl          12 KB  conda-forge\n",
      "    scikit-learn-0.24.2        |   py39h4dfa638_0         7.6 MB  conda-forge\n",
      "    scipy-1.5.3                |   py39hee8e79c_0        19.3 MB  conda-forge\n",
      "    threadpoolctl-3.1.0        |     pyh8a188c0_0          18 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        28.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  joblib             conda-forge/noarch::joblib-1.2.0-pyhd8ed1ab_0\n",
      "  libblas            conda-forge/linux-64::libblas-3.9.0-12_linux64_mkl\n",
      "  libcblas           conda-forge/linux-64::libcblas-3.9.0-12_linux64_mkl\n",
      "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-12.2.0-h69a702a_19\n",
      "  libgfortran5       conda-forge/linux-64::libgfortran5-12.2.0-h337968e_19\n",
      "  liblapack          conda-forge/linux-64::liblapack-3.9.0-12_linux64_mkl\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.9-2_cp39\n",
      "  scikit-learn       conda-forge/linux-64::scikit-learn-0.24.2-py39h4dfa638_0\n",
      "  scipy              conda-forge/linux-64::scipy-1.5.3-py39hee8e79c_0\n",
      "  threadpoolctl      conda-forge/noarch::threadpoolctl-3.1.0-pyh8a188c0_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2022.3.29-~ --> conda-forge::ca-certificates-2022.12.7-ha878542_0\n",
      "  certifi            pkgs/main/linux-64::certifi-2021.10.8~ --> conda-forge/noarch::certifi-2022.12.7-pyhd8ed1ab_0\n",
      "  openssl                                 1.1.1n-h7f8727e_0 --> 1.1.1t-h7f8727e_0\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? "
     ]
    }
   ],
   "source": [
    "!conda install scikit-learn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
