{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3119760/537395398.py:5: DtypeWarning: Columns (19,20,21,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  usdf = pd.read_csv(os.path.join(preprocessed_folder_path, 'us.csv'))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "preprocessing_folder_path = '/home/local/PSYCH-ADS/xuqian_chen/Github/twitter_entropy/code/preprocessing/'\n",
    "preprocessed_folder_path = '/home/local/PSYCH-ADS/xuqian_chen/Github/twitter_entropy/data/preprocessed/'\n",
    "usdf = pd.read_csv(os.path.join(preprocessed_folder_path, 'us.csv'))\n",
    "jpdf = pd.read_csv(os.path.join(preprocessed_folder_path, 'jp.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random 10000 sample from usdf and jpdf\n",
    "\n",
    "usdf = usdf.sample(n=10000, random_state=1)\n",
    "jpdf = jpdf.sample(n=10000, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entropy calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_en_nrc_emotion_lexicon(txt_path):\n",
    "    with open(txt_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    data = [line.strip().split('\\t') for line in lines]\n",
    "    df = pd.DataFrame(data, columns=[\"word\", \"emotion\", \"value\"])\n",
    "    df[\"value\"] = df[\"value\"].astype(int)\n",
    "    \n",
    "    df = df[df[\"value\"] == 1]\n",
    "    emotion_dict = {emotion: set(df[df[\"emotion\"] == emotion][\"word\"]) for emotion in df[\"emotion\"].unique()}\n",
    "    \n",
    "    return emotion_dict\n",
    "\n",
    "# path_to_english_nrc = os.path.join(preprocessing_folder_path, 'NRC-Emotion-Lexicon-Wordlevel-v0.92.txt')\n",
    "path_to_english_nrc = '/home/local/PSYCH-ADS/xuqian_chen/Github/twitter_entropy/data/preprocessing/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "english_nrc_emotion_dict = load_en_nrc_emotion_lexicon(path_to_english_nrc)\n",
    "\n",
    "\n",
    "def load_jp_nrc_emotion_lexicon(file_path):\n",
    "    emotion_dict = {\"anger\": set(), \"anticipation\": set(), \"disgust\": set(), \"fear\": set(), \"joy\": set(),\n",
    "                    \"negative\": set(), \"positive\": set(), \"sadness\": set(), \"surprise\": set(), \"trust\": set()}\n",
    "\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        next(f)  # Skip the header line\n",
    "        for line in f:\n",
    "            split_line = line.strip().split(\"\\t\")\n",
    "            japanese_word = split_line[-1]\n",
    "            emotions = [emotion for emotion, value in zip(emotion_dict.keys(), split_line[1:-1]) if int(value) == 1]\n",
    "\n",
    "            for emotion in emotions:\n",
    "                emotion_dict[emotion].add(japanese_word.lower())\n",
    "\n",
    "    return emotion_dict\n",
    "\n",
    "# path_to_japanese_nrc = os.path.join(preprocessing_folder_path, 'Japanese-NRC-EmoLex.txt')\n",
    "path_to_japanese_nrc = '/home/local/PSYCH-ADS/xuqian_chen/Github/twitter_entropy/data/preprocessing/Japanese-NRC-EmoLex.txt'\n",
    "japanese_nrc_emotion_dict = load_jp_nrc_emotion_lexicon(path_to_japanese_nrc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT IN USE: roberta senetiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 22:26:10.963327: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-20 22:26:10.963347: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion loadings: [0.9440009593963623]\n",
      "Emotion entropy: 1.292481250360578\n",
      "Emotion loadings: [0.9440009593963623]\n",
      "Emotion entropy: -0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "def get_emotion_loadings(tweet_text):\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    emotion_classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "    result = emotion_classifier(tweet_text)\n",
    "\n",
    "    # Extract emotion probabilities\n",
    "    emotion_probabilities = [entry[\"score\"] for entry in result]\n",
    "    return emotion_probabilities\n",
    "\n",
    "# Example usage\n",
    "tweet_text = \"I love this new phone! It's amazing.\"\n",
    "emotion_loadings = get_emotion_loadings(tweet_text)\n",
    "print(\"Emotion loadings:\", emotion_loadings)\n",
    "\n",
    "def emotion_entropy(post_emotions, threshold=0.5):\n",
    "    binary_emotions = [1 if emotion >= threshold else 0 for emotion in post_emotions]\n",
    "    probabilities = [emotion_count / len(post_emotions) for emotion_count in binary_emotions]\n",
    "    \n",
    "    entropy = -np.sum([p * np.log2(p) if p > 0 else 0 for p in probabilities])\n",
    "    return entropy\n",
    "\n",
    "# Example usage\n",
    "post_emotions = [0.7, 0.2, 0.8, 0.4, 0.6, 0.1]\n",
    "entropy = emotion_entropy(post_emotions)\n",
    "print(\"Emotion entropy:\", entropy)\n",
    "\n",
    "tweet_text = \"I love this new phone! It's amazing.\"\n",
    "emotion_loadings = get_emotion_loadings(tweet_text)\n",
    "entropy = emotion_entropy(emotion_loadings)\n",
    "print(\"Emotion loadings:\", emotion_loadings)\n",
    "print(\"Emotion entropy:\", entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "basic_emotions = ['anger', 'fear', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "\n",
    "ja_basic_emotion_concepts = {\n",
    "    emotion: list(words) for emotion, words in japanese_nrc_emotion_dict.items() if emotion in basic_emotions\n",
    "}\n",
    "en_basic_emotion_concepts = {\n",
    "    emotion: list(words) for emotion, words in english_nrc_emotion_dict.items() if emotion in basic_emotions\n",
    "}\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "# check if the fastText embeddings are downloaded\n",
    "import os\n",
    "\n",
    "if not os.path.isfile('cc.ja.300.bin'):\n",
    "    fasttext.util.download_model('ja', if_exists='ignore') \n",
    "if not os.path.isfile('cc.en.300.bin'):\n",
    "    fasttext.util.download_model('en', if_exists='ignore')\n",
    "\n",
    "ja_embeddings = fasttext.load_model('cc.ja.300.bin')\n",
    "en_embeddings = fasttext.load_model('cc.en.300.bin')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function: english emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fear': 0.41364262, 'sadness': 0.42352962, 'anger': 0.42670828, 'surprise': 0.48255166, 'disgust': 0.39483714, 'joy': 0.44914478}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader as api\n",
    "\n",
    "# load pre-trained fastText embeddings\n",
    "def load_fasttext_embeddings(model_name):\n",
    "    model = api.load(model_name)\n",
    "    # Get the mean vector for the list of words\n",
    "    # mean_vector = np.mean([model1[word] for word in wordlist if word in word_vectors], axis=0)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "def load_glove_embeddings(embeddings_file):\n",
    "    embeddings = {}\n",
    "    with open(embeddings_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Vectorize the dictionary of moral and emotional concepts\n",
    "def vectorize_concepts(concepts, embeddings):\n",
    "    concept_vectors = {}\n",
    "    for concept, words in concepts.items():\n",
    "        vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "        if vectors:  # Checking if the list is not empty\n",
    "            concept_vectors[concept] = np.mean(vectors, axis=0)\n",
    "        \n",
    "    return concept_vectors\n",
    "\n",
    "\n",
    "# Vectorize the input text\n",
    "def vectorize_text(text, embeddings):\n",
    "    tokens = word_tokenize(text)\n",
    "    vectors = [embeddings[token] for token in tokens if token in embeddings and token is not None]\n",
    "    if vectors:  # Checking if the list is not empty\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "# Calculate cosine similarity between input text and moral and emotional concepts\n",
    "def calculate_similarities(text_vector, concept_vectors):\n",
    "    similarities = {}\n",
    "    if text_vector is not None:\n",
    "        for concept, vector in concept_vectors.items():\n",
    "            similarities[concept] = cosine_similarity(text_vector.reshape(1, -1), vector.reshape(1, -1))[0][0]\n",
    "    else:\n",
    "        for concept, vector in concept_vectors.items():\n",
    "            similarities[concept] = np.nan\n",
    "    return similarities\n",
    "\n",
    "# Example usage:\n",
    "# glove_embeddings_file = 'glove.6B.300d.txt'\n",
    "# glove_embeddings = load_glove_embeddings(glove_embeddings_file)\n",
    "# en_embeddings = load_fasttext_embeddings(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "# Define your moral and emotional concepts dictionary here\n",
    "en_concept_vectors = vectorize_concepts(en_basic_emotion_concepts, en_embeddings)\n",
    "\n",
    "text = \"This is an example text to analyze moral and emotional content.\"\n",
    "text_vector = vectorize_text(text, en_embeddings)\n",
    "\n",
    "similarities = calculate_similarities(text_vector, en_concept_vectors)\n",
    "print(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_basic_emotion_concepts)\n",
    "len(en_concept_vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jp emotions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT IN USE: Tokenize the input text using Janome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fear': 0.28294072, 'sadness': 0.28086483, 'anger': 0.28085762, 'surprise': 0.30233887, 'disgust': 0.28768104, 'joy': 0.27653188}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# Load pre-trained FastText embeddings\n",
    "def load_fasttext_embeddings(embeddings_file):\n",
    "    embeddings = {}\n",
    "    with open(embeddings_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Vectorize the dictionary of moral and emotional concepts\n",
    "def vectorize_concepts(concepts, embeddings):\n",
    "    concept_vectors = {}\n",
    "    for concept, words in concepts.items():\n",
    "        vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "        concept_vectors[concept] = np.mean(vectors, axis=0)\n",
    "    return concept_vectors\n",
    "\n",
    "# Tokenize the input text using Janome\n",
    "def tokenize_japanese_text(text):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokens = tokenizer.tokenize(text, wakati=True)\n",
    "    return tokens\n",
    "\n",
    "# Vectorize the input text\n",
    "def vectorize_text(tokens, embeddings):\n",
    "    vectors = [embeddings[token] for token in tokens if token in embeddings]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Calculate cosine similarity between input text and moral and emotional concepts\n",
    "def calculate_similarities(text_vector, concept_vectors):\n",
    "    similarities = {}\n",
    "    for concept, vector in concept_vectors.items():\n",
    "        similarities[concept] = cosine_similarity(text_vector.reshape(1, -1), vector.reshape(1, -1))[0][0]\n",
    "    return similarities\n",
    "\n",
    "# ENTROPY\n",
    "def emotion_entropy(post_emotions, threshold=0.5):\n",
    "    binary_emotions = [1 if emotion >= threshold else 0 for emotion in post_emotions]\n",
    "    probabilities = [emotion_count / len(post_emotions) for emotion_count in binary_emotions]\n",
    "    \n",
    "    entropy = -np.sum([p * np.log2(p) if p > 0 else 0 for p in probabilities])\n",
    "    return entropy\n",
    "# Define your moral and emotional concepts dictionary here\n",
    "concepts = en_basic_emotion_concepts\n",
    "\n",
    "concept_vectors = vectorize_concepts(concepts, en_embeddings)\n",
    "\n",
    "text = \"His success is wonderful, but at the same time, I feel a bit jealous.\"\n",
    "text_vector = vectorize_text(text, en_embeddings)\n",
    "\n",
    "similarities = calculate_similarities(text_vector, concept_vectors)\n",
    "\n",
    "entropy = emotion_entropy(list(similarities.values()))\n",
    "print(similarities)\n",
    "print(entropy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': 0.6377529, 'disgust': 0.611441, 'fear': 0.62469923, 'joy': 0.6755974, 'sadness': 0.6264249, 'surprise': 0.67802525}\n",
      "2.584962500721156\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "def tokenize_japanese_text_mecab(text):\n",
    "    mecab = MeCab.Tagger(\"-Owakati\")\n",
    "    return mecab.parse(text).strip().split()\n",
    "\n",
    "def vectorize_jp_concepts(concepts, ja_embeddings):\n",
    "    concept_vectors = {}\n",
    "    for concept, words in concepts.items():\n",
    "        vectors = [ja_embeddings.get_word_vector(word) for word in words if word is not None]\n",
    "        if vectors:  # Checking if the list is not empty\n",
    "            concept_vectors[concept] = np.mean(vectors, axis=0)\n",
    "    return concept_vectors\n",
    "\n",
    "# Vectorize the input text\n",
    "def vectorize_jp_text(text, ja_embeddings):\n",
    "    tokens = mecab.parse(text).strip().split()\n",
    "    vectors =[ja_embeddings.get_word_vector(token) for token in tokens if token is not None]\n",
    "    if vectors:  # Checking if the list is not empty\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "def emotion_entropy(post_emotions, threshold=0.5):\n",
    "    binary_emotions = [1 if emotion >= threshold else 0 for emotion in post_emotions]\n",
    "    probabilities = [emotion_count / len(post_emotions) for emotion_count in binary_emotions]\n",
    "    \n",
    "    entropy = -np.sum([p * np.log2(p) if p > 0 else 0 for p in probabilities])\n",
    "    return entropy\n",
    "\n",
    "def calculate_mixed_emotion_entropy(sentence, embeddings,concept_vectors, language):\n",
    "    if language == 'ja':\n",
    "        \n",
    "        text_vector = vectorize_jp_text(sentence, embeddings)\n",
    "        \n",
    "    else:\n",
    "        text_vector = vectorize_text(sentence, embeddings)\n",
    "    similarities = calculate_similarities(text_vector, concept_vectors)\n",
    "    entropy = emotion_entropy(list(similarities.values()))\n",
    "    return similarities, entropy\n",
    "\n",
    "# Example usage:\n",
    "# fasttext_embeddings_file = 'cc.ja.300.vec'\n",
    "# fasttext_embeddings = load_fasttext_embeddings(fasttext_embeddings_file)\n",
    "\n",
    "# Define your moral and emotional concepts dictionary here\n",
    "ja_concept_vectors = vectorize_jp_concepts(ja_basic_emotion_concepts, ja_embeddings)\n",
    "\n",
    "# concept_vectors = vectorize_concepts(concepts, ja_embeddings)\n",
    "\n",
    "text = \"これは道徳的および感情的なコンテンツを分析するための例文です\"\n",
    "\n",
    "text_vector = vectorize_jp_text(text, ja_embeddings)\n",
    "\n",
    "similarities = calculate_similarities(text_vector, ja_concept_vectors)\n",
    "\n",
    "entropy = emotion_entropy(list(similarities.values()))\n",
    "print(similarities)\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm so happy to see you, but I'm sad that you have to leave soon. {'fear': 0.37510112, 'sadness': 0.4106004, 'anger': 0.3957127, 'surprise': 0.45528018, 'disgust': 0.38073587, 'joy': 0.43462116} 0\n",
      "I'm anxious about the exam, but I'm also excited about the challenge. {'fear': 0.35617098, 'sadness': 0.38363206, 'anger': 0.37038344, 'surprise': 0.41533512, 'disgust': 0.35978395, 'joy': 0.39620787} 0\n",
      "I'm surprised and disappointed by the news. {'fear': 0.37021956, 'sadness': 0.39169207, 'anger': 0.38696086, 'surprise': 0.4206731, 'disgust': 0.36965284, 'joy': 0.38753808} 0\n"
     ]
    }
   ],
   "source": [
    "en_mixed_emotion_sentences = [\n",
    "    \"I'm so happy to see you, but I'm sad that you have to leave soon.\",\n",
    "    \n",
    "    \"I'm anxious about the exam, but I'm also excited about the challenge.\",\n",
    "    \n",
    "    \"I'm surprised and disappointed by the news.\",\n",
    "]\n",
    "\n",
    "ja_mixed_emotion_sentences = [\n",
    "    \"彼の成功は素晴らしいけれども、同時に少し嫉妬心がある\",\n",
    "    \"仕事が終わったので安心だが、週末のパーティには緊張している\",\n",
    "    \"彼女の発表には驚いたが、ちょっと怒りも感じた\",\n",
    "]\n",
    "\n",
    "\n",
    "for sentence in en_mixed_emotion_sentences:\n",
    "    similarities, entropy = calculate_mixed_emotion_entropy(sentence, en_embeddings,en_concept_vectors,'en')\n",
    "    print(sentence, similarities, entropy)\n",
    "\n",
    "# for sentence in ja_mixed_emotion_sentences:\n",
    "#     similarities, entropy = calculate_mixed_emotion_entropy(sentence, ja_embeddings, language='ja')\n",
    "#     print(sentence, similarities, entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         6396 function calls in 0.008 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        3    0.000    0.000    0.000    0.000 1999364120.py:42(emotion_entropy)\n",
      "        3    0.000    0.000    0.000    0.000 1999364120.py:43(<listcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 1999364120.py:44(<listcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 1999364120.py:46(<listcomp>)\n",
      "        1    0.000    0.000    0.008    0.008 2842563575.py:3(test)\n",
      "        3    0.000    0.000    0.003    0.001 2887436742.py:34(vectorize_text)\n",
      "        3    0.000    0.000    0.001    0.000 2887436742.py:36(<listcomp>)\n",
      "        3    0.000    0.000    0.005    0.002 2887436742.py:40(calculate_similarities)\n",
      "        3    0.000    0.000    0.008    0.003 61706619.py:15(calculate_mixed_emotion_entropy)\n",
      "       36    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(einsum)\n",
      "       36    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(may_share_memory)\n",
      "        3    0.000    0.000    0.001    0.000 <__array_function__ internals>:177(mean)\n",
      "       75    0.000    0.000    0.001    0.000 <__array_function__ internals>:177(sum)\n",
      "       72    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:398(parent)\n",
      "        1    0.000    0.000    0.008    0.008 <string>:1(<module>)\n",
      "       44    0.000    0.000    0.000    0.000 FastText.py:115(get_dimension)\n",
      "       44    0.000    0.000    0.000    0.000 FastText.py:120(get_word_vector)\n",
      "       44    0.000    0.000    0.000    0.000 FastText.py:371(words)\n",
      "       44    0.000    0.000    0.001    0.000 FastText.py:383(__getitem__)\n",
      "       44    0.001    0.000    0.001    0.000 FastText.py:386(__contains__)\n",
      "        3    0.000    0.000    0.001    0.000 __init__.py:114(word_tokenize)\n",
      "        3    0.000    0.000    0.000    0.000 __init__.py:130(<listcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 __init__.py:96(sent_tokenize)\n",
      "      198    0.000    0.000    0.000    0.000 _base.py:1301(isspmatrix)\n",
      "       72    0.000    0.000    0.000    0.000 _config.py:16(_get_threadlocal_config)\n",
      "       72    0.000    0.000    0.000    0.000 _config.py:24(get_config)\n",
      "       36    0.000    0.000    0.003    0.000 _data.py:1733(normalize)\n",
      "       36    0.000    0.000    0.000    0.000 _data.py:84(_handle_zeros_in_scale)\n",
      "        3    0.000    0.000    0.001    0.000 _methods.py:162(_mean)\n",
      "        3    0.000    0.000    0.000    0.000 _methods.py:66(_count_reduce_items)\n",
      "      108    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
      "       72    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)\n",
      "        3    0.000    0.000    0.000    0.000 compat.py:25(add_py3_data)\n",
      "        3    0.000    0.000    0.000    0.000 data.py:119(split_resource_url)\n",
      "        3    0.000    0.000    0.000    0.000 data.py:146(normalize_resource_url)\n",
      "        3    0.000    0.000    0.000    0.000 data.py:202(normalize_resource_name)\n",
      "        3    0.000    0.000    0.000    0.000 data.py:662(load)\n",
      "        3    0.000    0.000    0.000    0.000 destructive.py:120(tokenize)\n",
      "      180    0.000    0.000    0.000    0.000 einsumfunc.py:989(_einsum_dispatcher)\n",
      "       36    0.000    0.000    0.000    0.000 einsumfunc.py:997(einsum)\n",
      "       18    0.000    0.000    0.000    0.000 extmath.py:120(safe_sparse_dot)\n",
      "       36    0.000    0.000    0.000    0.000 extmath.py:51(row_norms)\n",
      "       72    0.000    0.000    0.001    0.000 extmath.py:869(_safe_accumulator_op)\n",
      "       75    0.000    0.000    0.000    0.000 fromnumeric.py:2155(_sum_dispatcher)\n",
      "       75    0.000    0.000    0.001    0.000 fromnumeric.py:2160(sum)\n",
      "        3    0.000    0.000    0.000    0.000 fromnumeric.py:3351(_mean_dispatcher)\n",
      "        3    0.000    0.000    0.001    0.000 fromnumeric.py:3356(mean)\n",
      "       75    0.000    0.000    0.000    0.000 fromnumeric.py:69(_wrapreduction)\n",
      "       75    0.000    0.000    0.000    0.000 fromnumeric.py:70(<dictcomp>)\n",
      "       36    0.000    0.000    0.000    0.000 getlimits.py:457(__new__)\n",
      "       36    0.000    0.000    0.000    0.000 multiarray.py:1368(may_share_memory)\n",
      "       36    0.000    0.000    0.000    0.000 numeric.py:1859(isscalar)\n",
      "      144    0.000    0.000    0.000    0.000 numerictypes.py:282(issubclass_)\n",
      "       72    0.000    0.000    0.000    0.000 numerictypes.py:356(issubdtype)\n",
      "       18    0.000    0.000    0.005    0.000 pairwise.py:1216(cosine_similarity)\n",
      "       18    0.000    0.000    0.000    0.000 pairwise.py:39(_return_float_dtype)\n",
      "       18    0.000    0.000    0.002    0.000 pairwise.py:63(check_pairwise_arrays)\n",
      "        3    0.000    0.000    0.000    0.000 posixpath.py:334(normpath)\n",
      "        3    0.000    0.000    0.000    0.000 posixpath.py:41(_get_sep)\n",
      "        3    0.000    0.000    0.000    0.000 posixpath.py:60(isabs)\n",
      "        3    0.000    0.000    0.000    0.000 punkt.py:1277(tokenize)\n",
      "        6    0.000    0.000    0.000    0.000 punkt.py:1319(span_tokenize)\n",
      "        3    0.000    0.000    0.000    0.000 punkt.py:1332(sentences_from_text)\n",
      "        3    0.000    0.000    0.000    0.000 punkt.py:1341(<listcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 punkt.py:1354(_match_potential_end_contexts)\n",
      "        6    0.000    0.000    0.000    0.000 punkt.py:1429(_slices_from_text)\n",
      "        6    0.000    0.000    0.000    0.000 punkt.py:1443(_realign_boundaries)\n",
      "        3    0.000    0.000    0.000    0.000 punkt.py:282(period_context_re)\n",
      "        6    0.000    0.000    0.000    0.000 punkt.py:313(_pair_iter)\n",
      "        3    0.000    0.000    0.000    0.000 re.py:198(search)\n",
      "        3    0.000    0.000    0.000    0.000 re.py:203(sub)\n",
      "        6    0.000    0.000    0.000    0.000 re.py:289(_compile)\n",
      "       81    0.000    0.000    0.000    0.000 re.py:325(_subx)\n",
      "       13    0.000    0.000    0.000    0.000 re.py:331(filter)\n",
      "       13    0.000    0.000    0.000    0.000 sre_parse.py:1066(expand_template)\n",
      "       72    0.000    0.000    0.000    0.000 validation.py:254(_num_samples)\n",
      "       72    0.000    0.000    0.000    0.000 validation.py:484(_ensure_no_complex_data)\n",
      "       72    0.001    0.000    0.003    0.000 validation.py:494(check_array)\n",
      "       72    0.000    0.000    0.002    0.000 validation.py:90(_assert_all_finite)\n",
      "       72    0.000    0.000    0.000    0.000 warnings.py:165(simplefilter)\n",
      "       72    0.000    0.000    0.000    0.000 warnings.py:181(_add_filter)\n",
      "       72    0.000    0.000    0.000    0.000 warnings.py:437(__init__)\n",
      "       72    0.000    0.000    0.000    0.000 warnings.py:458(__enter__)\n",
      "       72    0.000    0.000    0.000    0.000 warnings.py:477(__exit__)\n",
      "      108    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "       72    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "      216    0.000    0.000    0.000    0.000 {built-in method _warnings._filters_mutated}\n",
      "        1    0.000    0.000    0.008    0.008 {built-in method builtins.exec}\n",
      "       75    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "      648    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "      867    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "      222    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "       96    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "       80    0.000    0.000    0.000    0.000 {built-in method numpy.array}\n",
      "       75    0.001    0.000    0.001    0.000 {built-in method numpy.asanyarray}\n",
      "       72    0.000    0.000    0.000    0.000 {built-in method numpy.asarray}\n",
      "       36    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.c_einsum}\n",
      "      150    0.000    0.000    0.002    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method posix.fspath}\n",
      "        9    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "       72    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'finditer' of 're.Pattern' objects}\n",
      "       42    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "       32    0.000    0.000    0.000    0.000 {method 'group' of 're.Match' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'index' of 'str' objects}\n",
      "       72    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
      "       78    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "       19    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
      "       78    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "       72    0.000    0.000    0.000    0.000 {method 'remove' of 'list' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
      "       36    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "       72    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'search' of 're.Pattern' objects}\n",
      "       12    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
      "       12    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "       96    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "\n",
    "def test():\n",
    "    for sentence in en_mixed_emotion_sentences:\n",
    "        similarities, entropy = calculate_mixed_emotion_entropy(sentence, en_embeddings,en_concept_vectors,'en')\n",
    "\n",
    "\n",
    "    # for sentence in ja_mixed_emotion_sentences:\n",
    "        # similarities, entropy = calculate_mixed_emotion_entropy(sentence, ja_embeddings, ja_concept_vectors, language='ja')\n",
    "\n",
    "\n",
    "cProfile.run('test()')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm thrilled to have received the promotion, but I'm also nervous about the new responsibilities that come with it. {'fear': 0.4017118, 'sadness': 0.42628857, 'anger': 0.41740066, 'surprise': 0.47821122, 'disgust': 0.39795023, 'joy': 0.45706582} 0\n",
      "I was relieved that the medical test came back negative, but I'm still worried about my persistent symptoms. {'fear': 0.43635684, 'sadness': 0.4685515, 'anger': 0.44494742, 'surprise': 0.49110755, 'disgust': 0.43099707, 'joy': 0.45792314} 0\n",
      "I'm excited to start this new chapter in my life, but I'm sad about leaving my friends and family behind. {'fear': 0.41011912, 'sadness': 0.44670117, 'anger': 0.42168623, 'surprise': 0.48029298, 'disgust': 0.40653467, 'joy': 0.47900432} 0\n",
      "It's heartbreaking to say goodbye to my colleagues, but I am eager to explore the new job opportunity. {'fear': 0.39935204, 'sadness': 0.4212766, 'anger': 0.41590917, 'surprise': 0.48764575, 'disgust': 0.3840256, 'joy': 0.47415853} 0\n",
      "I'm proud of my children for becoming independent, but I'm also feeling a little lonely now that they've moved out. {'fear': 0.42065212, 'sadness': 0.4545647, 'anger': 0.43409202, 'surprise': 0.48321915, 'disgust': 0.42299446, 'joy': 0.47534966} 0\n",
      "I feel joyous for my friend's wedding, but at the same time, I'm feeling a bit envious. {'fear': 0.41491565, 'sadness': 0.4469679, 'anger': 0.43043363, 'surprise': 0.49351895, 'disgust': 0.41296643, 'joy': 0.48345384} 0\n",
      "昇進してとてもうれしいですが、新しい責任にも少し緊張しています。 {'anger': 0.520053, 'disgust': 0.488768, 'fear': 0.5063507, 'joy': 0.5754167, 'sadness': 0.5252794, 'surprise': 0.5814836} 2.15413541726763\n",
      "医療検査が陰性でほっとしていますが、持続的な症状についてはまだ心配しています。 {'anger': 0.549001, 'disgust': 0.53207624, 'fear': 0.54877347, 'joy': 0.5841476, 'sadness': 0.56324184, 'surprise': 0.60204244} 2.584962500721156\n",
      "新しい生活の章を開始するのが楽しみですが、友人や家族と離れることは悲しいです。 {'anger': 0.6087129, 'disgust': 0.57587504, 'fear': 0.6005447, 'joy': 0.6765133, 'sadness': 0.61046803, 'surprise': 0.6691774} 2.584962500721156\n",
      "同僚に別れを告げるのは心が痛むけれど、新たな仕事の機会を探求することには前向きです。 {'anger': 0.65027905, 'disgust': 0.61667764, 'fear': 0.6407017, 'joy': 0.7179838, 'sadness': 0.6533702, 'surprise': 0.7112208} 2.584962500721156\n",
      "子どもたちが自立してくれて誇らしいですが、彼らが引っ越してからは少し寂しい気もします。 {'anger': 0.5298428, 'disgust': 0.5031283, 'fear': 0.5180982, 'joy': 0.59123564, 'sadness': 0.53556544, 'surprise': 0.58991086} 2.584962500721156\n",
      "友人の結婚に喜んでいる一方で、少し羨ましい気もします。 {'anger': 0.54840636, 'disgust': 0.52543926, 'fear': 0.5327827, 'joy': 0.61970747, 'sadness': 0.55602044, 'surprise': 0.6119412} 2.584962500721156\n"
     ]
    }
   ],
   "source": [
    "mixed_emotion_sentences = {\n",
    "    \"sentence1\": \"I'm thrilled to have received the promotion, but I'm also nervous about the new responsibilities that come with it.\",\n",
    "    \"sentence2\": \"I was relieved that the medical test came back negative, but I'm still worried about my persistent symptoms.\",\n",
    "    \"sentence3\": \"I'm excited to start this new chapter in my life, but I'm sad about leaving my friends and family behind.\",\n",
    "    \"sentence4\": \"It's heartbreaking to say goodbye to my colleagues, but I am eager to explore the new job opportunity.\",\n",
    "    \"sentence5\": \"I'm proud of my children for becoming independent, but I'm also feeling a little lonely now that they've moved out.\",\n",
    "    \"sentence6\": \"I feel joyous for my friend's wedding, but at the same time, I'm feeling a bit envious.\",\n",
    "}\n",
    "en_mixed_emotion_sentences = list(mixed_emotion_sentences.values())\n",
    "\n",
    "ja_mixed_emotion_sentences= [\n",
    "    \"昇進してとてもうれしいですが、新しい責任にも少し緊張しています。\",\n",
    "    \"医療検査が陰性でほっとしていますが、持続的な症状についてはまだ心配しています。\",\n",
    "    \"新しい生活の章を開始するのが楽しみですが、友人や家族と離れることは悲しいです。\",\n",
    "    \"同僚に別れを告げるのは心が痛むけれど、新たな仕事の機会を探求することには前向きです。\",\n",
    "    \"子どもたちが自立してくれて誇らしいですが、彼らが引っ越してからは少し寂しい気もします。\",\n",
    "    \"友人の結婚に喜んでいる一方で、少し羨ましい気もします。\",\n",
    "]\n",
    "\n",
    "\n",
    "for sentence in en_mixed_emotion_sentences:\n",
    "    similarities, entropy = calculate_mixed_emotion_entropy(sentence, en_embeddings,en_concept_vectors,'en')\n",
    "    print(sentence, similarities, entropy)\n",
    "\n",
    "for sentence in ja_mixed_emotion_sentences:\n",
    "    similarities, entropy = calculate_mixed_emotion_entropy(sentence, ja_embeddings,ja_concept_vectors, language='ja')\n",
    "    print(sentence, similarities, entropy)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# caculate tweets entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "entropies = []\n",
    "angers = []\n",
    "fears = []\n",
    "joys = []\n",
    "sadnesses = []\n",
    "disgusts = []\n",
    "surprises = []\n",
    "\n",
    "for t in tqdm(usdf['text']):\n",
    "    # similarities = calculate_similarities(text_vector, concept_vectors)\n",
    "    try:\n",
    "        similarities, entroppy = calculate_mixed_emotion_entropy(t, en_embeddings,en_concept_vectors,'en')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(t)\n",
    "        \n",
    "        # Input\n",
    "        similarities = {'anger':np.nan,'fear':np.nan,'joy':np.nan,'sadness':np.nan,'disgust':np.nan,'surprise':np.nan}\n",
    "        entropy = np.nan\n",
    "    entropies.append(entropy)\n",
    "    angers.append(similarities['anger'])\n",
    "    fears.append(similarities['fear'])\n",
    "    joys.append(similarities['joy'])\n",
    "    sadnesses.append(similarities['sadness'])\n",
    "    disgusts.append(similarities['disgust'])\n",
    "    surprises.append(similarities['surprise'])\n",
    "\n",
    "# usdf['entropy'] = entropies\n",
    "usdf['anger'] = angers\n",
    "usdf['fear'] = fears\n",
    "usdf['joy'] = joys\n",
    "usdf['sadness'] = sadnesses\n",
    "usdf['disgust'] = disgusts\n",
    "usdf['surprise'] = surprises\n",
    "usdf['entropy'] = entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:22<00:00, 31.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "def process_text_en(t):\n",
    "    try:\n",
    "        similarities, entropy = calculate_mixed_emotion_entropy(t, en_embeddings, en_concept_vectors, 'en')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Input\n",
    "        similarities = {'anger':np.nan, 'fear':np.nan, 'joy':np.nan, 'sadness':np.nan, 'disgust':np.nan, 'surprise':np.nan}\n",
    "        entropy = np.nan\n",
    "\n",
    "    return (similarities, entropy)\n",
    "\n",
    "with Pool(cpu_count()) as p:\n",
    "    results = list(tqdm(p.imap(process_text_en, usdf['text']), total=len(usdf)))\n",
    "\n",
    "# save results\n",
    "# import pickle\n",
    "# with open(os.path.join(preprocessing_folder_path, 'usdf_mixed_emotion_results.pkl'), 'wb') as f:\n",
    "#     pickle.dump(results, f)\n",
    "\n",
    "# Separate the results into their respective lists\n",
    "similarities_list, entropies = zip(*results)\n",
    "angers, fears, joys, sadnesses, disgusts, surprises = zip(*[s.values() for s in similarities_list])\n",
    "\n",
    "usdf['anger'] = angers\n",
    "usdf['fear'] = fears\n",
    "usdf['joy'] = joys\n",
    "usdf['sadness'] = sadnesses\n",
    "usdf['disgust'] = disgusts\n",
    "usdf['surprise'] = surprises\n",
    "usdf['entropy'] = entropies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_jp(t):\n",
    "    try:\n",
    "        similarities, entropy = calculate_mixed_emotion_entropy(t, ja_embeddings, ja_concept_vectors, 'ja')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Input\n",
    "        similarities = {'anger':np.nan, 'fear':np.nan, 'joy':np.nan, 'sadness':np.nan, 'disgust':np.nan, 'surprise':np.nan}\n",
    "        entropy = np.nan\n",
    "\n",
    "    return (similarities, entropy)\n",
    "\n",
    "with Pool(cpu_count()) as p:\n",
    "    results = list(tqdm(p.imap(process_text_jp, jpdf['text']), total=len(jpdf)))\n",
    "\n",
    "# Separate the results into their respective lists\n",
    "similarities_list, entropies = zip(*results)\n",
    "angers, fears, joys, sadnesses, disgusts, surprises = zip(*[s.values() for s in similarities_list])\n",
    "\n",
    "jpdf['anger'] = angers\n",
    "jpdf['fear'] = fears\n",
    "jpdf['joy'] = joys\n",
    "jpdf['sadness'] = sadnesses\n",
    "jpdf['disgust'] = disgusts\n",
    "jpdf['surprise'] = surprises\n",
    "jpdf['entropy'] = entropies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "usdf.to_csv(os.path.join(preprocessed_folder_path,\"us_entropy.csv\") index=False)\n",
    "jpdf.to_csv(os.path.join(preprocessed_folder_path,\"jp_entropy.csv\") , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the entropy of the two languages\n",
    "print(usdf['entropy'].mean())\n",
    "print(jpdf['entropy'].mean())\n",
    "\n",
    "# test which is higher\n",
    "from scipy.stats import ttest_ind\n",
    "ttest_ind(usdf['entropy'], jpdf['entropy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f669a5dd70341fe1cf9fa92e7b3b20791cc959f37db4cdb468169999145ea5f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
